{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable, grad\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pyDOE import lhs\n",
    "from utils import *\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN(nn.Module):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        super(PhysicsInformedNN, self).__init__()\n",
    "    \n",
    "        self.layers = layers\n",
    "\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        self.preds = None\n",
    "\n",
    "        self.loss = None\n",
    "\n",
    "        self.loss_list = []       \n",
    "        \n",
    "        self.optimizer = torch.optim.LBFGS(params=self.weights+self.biases,\n",
    "                                            lr=0.00001, max_iter=5000, max_eval=5000,\n",
    "                                         tolerance_grad=1e-07, tolerance_change=1e-08,\n",
    "                                          history_size=100, line_search_fn=None)\n",
    "\n",
    "\n",
    "    def initialize_NN(self, layers):        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0,num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = Variable(torch.zeros([1,layers[l+1]], dtype=torch.float32), requires_grad=True)\n",
    "            weights.append(W)\n",
    "            biases.append(b)        \n",
    "        return weights, biases\n",
    "        \n",
    "    def xavier_init(self, size):\n",
    "\n",
    "        return Variable(nn.init.xavier_normal_(torch.empty(size[0], size[1])), requires_grad=True)\n",
    "    \n",
    "    def neural_net(self, x, y, weights, biases):\n",
    "\n",
    "        num_layers = len(weights) + 1\n",
    "        H = torch.cat((x,y),1)\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = torch.tanh(torch.add(torch.matmul(H, W), b))\n",
    "\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = torch.add(torch.matmul(H, W), b) #.requires_grad_()\n",
    "\n",
    "        return Y\n",
    "\n",
    "    def net_u(self, x, y): # direct data match, including Dirichlet BCs\n",
    "        u = self.neural_net(x, y, self.weights, self.biases)\n",
    "        return u\n",
    "    \n",
    "    def net_du(self, x, y): # first-order derivative match, inlcuding Neumann BCs\n",
    "\n",
    "        u = self.neural_net(x, y, self.weights, self.biases)\n",
    "\n",
    "        u_x = grad(u.sum(), x, create_graph=True)[0]\n",
    "        u_y = grad(u.sum(), y, create_graph=True)[0]\n",
    "\n",
    "        return u_x.requires_grad_(True), u_y.requires_grad_(True)\n",
    "\n",
    "    def net_f(self, x, y): # general PDE match, usually formulated in higher-order\n",
    "\n",
    "        u_x, u_y = self.net_du(x, y)\n",
    "\n",
    "        u_yy = grad(u_y.sum(), y, create_graph=True)[0]\n",
    "\n",
    "        u_xx = grad(u_x.sum(), x, create_graph=True)[0]\n",
    "\n",
    "        f = u_yy + u_xx\n",
    "\n",
    "        return f.requires_grad_(True)\n",
    "\n",
    "    def forward(self, x_tensors, y_tensors, keys=None):\n",
    "\n",
    "        if keys is None:\n",
    "            keys = x_tensors.keys()\n",
    "        else:\n",
    "            preds = dict()\n",
    "            for i in keys:\n",
    "                preds[i] = None\n",
    "\n",
    "        for i in keys:\n",
    "\n",
    "            if i == 'nuem':\n",
    "                dudx_pred, _ = self.net_du(x_tensors[i], y_tensors[i])\n",
    "                preds[i] = dudx_pred\n",
    "\n",
    "            elif i == 'f':\n",
    "                f_pred = self.net_f(x_tensors[i], y_tensors[i])\n",
    "                preds[i] = f_pred\n",
    "\n",
    "            elif i == 'u':\n",
    "                u_pred = self.net_u(x_tensors[i], y_tensors[i]) \n",
    "                preds[i] = u_pred\n",
    "\n",
    "            elif i == 'diri':\n",
    "                diri_pred = self.net_u(x_tensors[i], y_tensors[i])   \n",
    "                preds[i] = diri_pred\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def loss_func(self, pred_dict, true_dict, weights=None):\n",
    "    \n",
    "        loss = torch.tensor(0.0, dtype=torch.float32)\n",
    "        keys = pred_dict.keys()\n",
    "\n",
    "        if weights is None:\n",
    "            weights = dict()\n",
    "            for i in keys:\n",
    "                weights[i] = 1.0\n",
    "\n",
    "        for i in keys:\n",
    "            res = pred_dict[i] - true_dict[i]\n",
    "            loss += weights[i]*torch.mean(res.pow(2))\n",
    "\n",
    "        return loss.requires_grad_()\n",
    "    \n",
    "    def customized_backward(self, loss, params):\n",
    "        grads = grad(loss, params, retain_graph=True)\n",
    "        for vid in range(len(params)):\n",
    "            params[vid].grad = grads[vid]\n",
    "        return grads\n",
    "\n",
    "    def unzip_train_dict(self, train_dict, keys=None):\n",
    "        if keys is None:\n",
    "            keys = train_dict.keys()\n",
    "\n",
    "        x_tensors = dict()\n",
    "        y_tensors = dict()\n",
    "        true_dict = dict()\n",
    "\n",
    "        for i in keys:\n",
    "            x_tensors[i] = train_dict[i][0]\n",
    "            y_tensors[i] = train_dict[i][1]\n",
    "            true_dict[i] = train_dict[i][2]\n",
    "\n",
    "        return (x_tensors, y_tensors, true_dict)\n",
    "\n",
    "    def train_LBFGS(self, train_dict, loss_func, optimizer):\n",
    "\n",
    "        (x_tensors, y_tensors, true_dict) = self.unzip_train_dict(train_dict)\n",
    "\n",
    "        def closure():\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_dict = self.forward(x_tensors, y_tensors, keys=('diri', 'nuem', 'f'))\n",
    "            loss = loss_func(pred_dict, true_dict) #.requires_grad_()\n",
    "            \n",
    "            self.callback(loss)\n",
    "            if np.remainder(len(self.loss_list),100) == 0:\n",
    "                print('Iter #', len(self.loss_list), 'Loss:', self.loss_list[-1].detach().numpy().squeeze())\n",
    "            \n",
    "            g = self.customized_backward(loss, self.weights+self.biases)\n",
    "            # loss.backward(retain_graph=True)\n",
    "\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        self.pred_dict = self.forward(x_tensors, y_tensors, keys=('diri', 'nuem', 'f'))\n",
    "        self.loss = loss_func(self.pred_dict, true_dict) #.requires_grad_()\n",
    "\n",
    "    def train(self, epoch, u_data, f_data, bc_data, loss_func, optimizer):\n",
    "        (x_tensors, y_tensors, true_dict) = self.unzip_train_dict(train_dict)\n",
    "\n",
    "\n",
    "        for i in range(epoch):\n",
    "            optimizer.zero_grad()\n",
    "            pred_dict = self.forward(x_tensors, y_tensors, keys=('diri', 'nuem', 'f'))\n",
    "            loss = loss_func(pred_dict, true_dict) #.requires_grad_()\n",
    "            \n",
    "            self.callback(loss)\n",
    "            if np.remainder(len(self.loss_list),100) == 0:\n",
    "                print('Iter #', len(self.loss_list), 'Loss:', self.loss_list[-1].detach().numpy().squeeze())\n",
    "            \n",
    "            g = self.customized_backward(loss, self.weights+self.biases)\n",
    "            # loss.backward()\n",
    "\n",
    "  \n",
    "            optimizer.step()\n",
    "\n",
    "        self.pred_dict = self.forward(x_tensors, y_tensors, keys=('diri', 'nuem', 'f'))\n",
    "        self.loss = loss_func(self.pred_dict, true_dict) #.requires_grad_()\n",
    "\n",
    "    def callback(self, loss):\n",
    "        self.loss_list.append(loss)\n",
    "\n",
    "    def coor_shift(self, X, lbs, ubs):\n",
    "\n",
    "        return 2.0*(X - lbs) / (ubs - lbs) - 1\n",
    "\n",
    "    def data_loader(self, X, u, lbs, ubs):\n",
    "                \n",
    "        X = self.coor_shift(X, lbs, ubs)\n",
    "\n",
    "        x_tensor = torch.tensor(X[:,0:1], requires_grad=True, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(X[:,1:2], requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "        u_tensor = torch.tensor(u, dtype=torch.float32)\n",
    "\n",
    "        return (x_tensor, y_tensor, u_tensor)\n",
    "\n",
    "    def predict(self, X_input):\n",
    "        x_tensor = torch.tensor(X_input[:,0:1], dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(X_input[:,1:2], dtype=torch.float32)\n",
    "        return self.neural_net(x_tensor, y_tensor, self.weights, self.biases).detach().numpy().squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_u = 100\n",
    "N_f = 20*20\n",
    "N_uc = 30\n",
    "\n",
    "nx = 61\n",
    "ny = 61\n",
    "\n",
    "x = np.linspace(0,1,nx)\n",
    "y = np.linspace(0,1,ny)\n",
    "\n",
    "Exact = p_analytical(x,y)\n",
    "\n",
    "X, Y = np.meshgrid(x,y)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], Y.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Domain bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)    \n",
    "lbs = np.array([0,0])\n",
    "ubs = np.array([1,1])\n",
    "    \n",
    "# Dirichlet BCs (top, left, bottom bounds)\n",
    "N_diri = 100 # No. of point for Dirichlet BCs\n",
    "\n",
    "# top\n",
    "xx1 = np.hstack((X[0:1,:].T, Y[0:1,:].T))\n",
    "uu1 = Exact[0:1,:].T\n",
    "# left\n",
    "xx2 = np.hstack((X[:,0:1], Y[:,0:1]))\n",
    "uu2 = Exact[:,0:1]\n",
    "# bottom\n",
    "xx3 = np.hstack((X[-1:,:].T, Y[-1:,:].T))\n",
    "uu3 = Exact[-1:,:].T\n",
    "\n",
    "X_diri_train = np.vstack([xx1, xx2, xx3])\n",
    "diri_train = np.vstack([uu1, uu2, uu3])\n",
    "X_diri_train, diri_train = random_choice_sample([X_diri_train, diri_train], N_diri)\n",
    "\n",
    "# Neumann BCs (right bound)\n",
    "N_nuem = 30  # No. of points for Neumann BCs\n",
    "\n",
    "xx4 = np.hstack((X[:,-1:], Y[:,-1:]))\n",
    "dudx4 = np.zeros((xx4.shape[0],1))\n",
    "X_nuem_train, nuem_train = random_choice_sample([xx4, dudx4], N_nuem)\n",
    "\n",
    "# direct measurement data, besides known BCs\n",
    "N_u = 100     # data match (direct measurement)\n",
    "xx_u = np.hstack((X[1:-1,1:-1].flatten()[:,None], Y[1:-1,1:-1].flatten()[:,None]))\n",
    "X_u_train, u_train = random_choice_sample([xx_u, np.expand_dims(Exact[1:-1,1:-1].flatten(), axis=1)], N_u)\n",
    "\n",
    "# formulated PDE data collocation\n",
    "N_f = 200\n",
    "X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
    "# X_f_train = np.vstack((X_f_train, X_u_train))\n",
    "f_train = np.zeros((X_f_train.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BCs = [1]\n",
    "\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "model = PhysicsInformedNN(layers)\n",
    "\n",
    "u_data = model.data_loader(X_u_train, u_train, lbs, ubs)\n",
    "f_data = model.data_loader(X_f_train, f_train, lbs, ubs)\n",
    "nuem_data = model.data_loader(X_nuem_train, nuem_train, lbs, ubs)\n",
    "diri_data = model.data_loader(X_diri_train, diri_train, lbs, ubs)\n",
    "# key:(data, loss_eval_weight) -> data: (x,y,val)\n",
    "train_dict = {\n",
    "    'u': u_data,\n",
    "    'f': f_data,\n",
    "    'nuem': nuem_data,\n",
    "    'diri': diri_data\n",
    "}\n",
    "\n",
    "optimizer = torch.optim.LBFGS(params=model.weights+model.biases,\n",
    "                                lr=0.001, max_iter=300, #max_eval=4000,\n",
    "                                tolerance_grad=1e-05, tolerance_change=1e-07,\n",
    "                                history_size=10, line_search_fn=None)\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.weights+model.biases, lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ed17d346105d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_LBFGS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# model.train(1000, train_dict, model.loss_func, optimizer)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0melapsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training time: %.4f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0melapsed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dict' is not defined"
     ]
    }
   ],
   "source": [
    "start_time = time.time() \n",
    "model.train_LBFGS(train_dict, model.loss_func, optimizer)\n",
    "# model.train(1000, train_dict, model.loss_func, optimizer)\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.4f' % (elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.coor_shift(X_star, lbs, ubs)\n",
    "u_pred = model.predict(X_pred)\n",
    "        \n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "print('Error u: %e' % (error_u))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('u_pred_torch.txt', u_pred)\n",
    "# u_pred = np.loadtxt('u_pred_torch.txt').reshape((nx,ny))\n",
    "u_pred = u_pred.reshape((nx,ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exact = p_analytical(x,y)\n",
    "\n",
    "plot_3D(x,y,u_pred, 'Prediction')\n",
    "plot_3D(x,y,Exact, 'True')\n",
    "# plot_map_2d(x, y, Exact-u_pred, (nx,ny))\n",
    "# compare_true_pred(Exact, u_pred, x, y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
